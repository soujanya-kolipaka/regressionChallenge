---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html: default
execute:
  echo: true
  eval: true
---

# üóëÔ∏è Regression Challenge - Linear Model Interpretability

## Introduction

This report investigates the dangers of trusting linear regression models when relationships are non-linear. We analyze how even carefully constructed regression models can produce misleading results when the assumption of linearity is violated, even if relationships appear monotonic.

**True Relationship:** We know that Anxiety = Stress + 0.1 √ó Time, where:
- Anxiety is measured by fMRI activity
- Stress is measured by cortisol level in blood
- Time is the number of minutes on social media in the last 24 hours

**True Coefficients:**
- Intercept ($\beta_0$) = 0
- Stress coefficient ($\beta_1$) = 1
- Time coefficient ($\beta_2$) = 0.1

**Key Problem:** In practice, we often can't measure stress directly with expensive blood tests. Instead, we use surveys (StressSurvey) as a proxy. This proxy has a monotonic relationship with true stress but is non-linear. We'll see how this non-linearity causes regression to fail.

### The Data Generation Process

```{python}
#| echo: false
#| include: false
import pandas as pd

# Generate the "true" data with known relationships
observDF = pd.DataFrame({
    'Stress': [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
    'StressSurvey': [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
    'Time': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
    'Anxiety': [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22]
})
```

```{python}
#| label: tbl-observations
#| tbl-cap: "Observed data with known true relationships"
#| echo: true
observDF
```

Notice that $Anxiety = Stress + 0.1 \times Time$ indeed holds perfectly. Also, notice the addition of a `StressSurvey` column. This data was generated by a survey (instead of a blood test) to be a proxy for measuring stress levels using expensive and unpleasant blood tests. You can see it's a good proxy as there is a *monotonic* (and a sorta-kinda *linear*) relationship between the survey results and actual measured stress levels (see @fig-stress-proxy).

```{python}
#| label: fig-stress-proxy
#| fig-cap: "StressSurvey as a proxy for actual Stress levels"
#| echo: false
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (7, 4)

# Create the plot
fig, ax = plt.subplots()
ax.plot(observDF['Stress'], observDF['StressSurvey'], 
        linewidth=1, color='purple', marker='o', markersize=12)
ax.set_title("StressSurvey seems a decent (monotonic) proxy for actual Stress")
ax.set_xlabel("Actual Stress Level")
ax.set_ylabel("Stress Survey Response")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

I. **Bivariate Regression Analysis with StressSurvey:** Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?


```{python}
#| echo: false
#| label: bivariate-regression-stresssurvey
#| fig-cap: "Bivariate regression of Anxiety on StressSurvey"
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare data for regression
X_stresssurvey = observDF[['StressSurvey']].values
y_anxiety = observDF['Anxiety'].values

# Manual calculation of regression coefficient
# Formula: Œ≤‚ÇÅ = Œ£(xi - xÃÑ)(yi - »≥) / Œ£(xi - xÃÑ)¬≤
x_mean = np.mean(X_stresssurvey)
y_mean = np.mean(y_anxiety)

# Calculate numerator: Œ£(xi - xÃÑ)(yi - »≥)
numerator = np.sum((X_stresssurvey.flatten() - x_mean) * (y_anxiety - y_mean))

# Calculate denominator: Œ£(xi - xÃÑ)¬≤
denominator = np.sum((X_stresssurvey.flatten() - x_mean) ** 2)

# Calculate coefficient manually
coefficient_manual = numerator / denominator

# Calculate intercept manually: Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅ * xÃÑ
intercept_manual = y_mean - coefficient_manual * x_mean

# Fit bivariate regression: Anxiety on StressSurvey (using sklearn for verification)
model_stresssurvey = LinearRegression()
model_stresssurvey.fit(X_stresssurvey, y_anxiety)

# Get coefficients from sklearn
intercept = model_stresssurvey.intercept_
coefficient = model_stresssurvey.coef_[0]
r_squared = r2_score(y_anxiety, model_stresssurvey.predict(X_stresssurvey))

# Display manual calculation
print("=" * 60)
print("MANUAL CALCULATION OF REGRESSION COEFFICIENT")
print("=" * 60)
print(f"Mean of StressSurvey (xÃÑ): {x_mean:.4f}")
print(f"Mean of Anxiety (»≥): {y_mean:.4f}")
print()
print(f"Numerator: Œ£(xi - xÃÑ)(yi - »≥) = {numerator:.4f}")
print(f"Denominator: Œ£(xi - xÃÑ)¬≤ = {denominator:.4f}")
print()
print(f"Manual calculation:")
print(f"  Coefficient (Œ≤‚ÇÅ) = {numerator:.4f} / {denominator:.4f} = {coefficient_manual:.4f}")
print(f"  Intercept (Œ≤‚ÇÄ) = {y_mean:.4f} - {coefficient_manual:.4f} √ó {x_mean:.4f} = {intercept_manual:.4f}")
print()
print(f"Sklearn verification:")
print(f"  Coefficient (Œ≤‚ÇÅ) = {coefficient:.4f}")
print(f"  Intercept (Œ≤‚ÇÄ) = {intercept:.4f}")
print("=" * 60)
print()

# Display results
print("=" * 60)
print("BIVARIATE REGRESSION: Anxiety on StressSurvey")
print("=" * 60)
print(f"Intercept (Œ≤‚ÇÄ): {intercept:.4f}")
print(f"Coefficient on StressSurvey (Œ≤‚ÇÅ): {coefficient:.4f}")
print(f"R-squared: {r_squared:.4f}")
print()
print("True Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: StressSurvey is a proxy for Stress")
print("True coefficient on Stress (if we had it): Œ≤‚ÇÅ = 1.0")
print()
print(f"Comparison:")
print(f"  Estimated coefficient: {coefficient:.4f}")
print(f"  True Stress coefficient: 1.0")
print(f"  Difference: {abs(coefficient - 1.0):.4f}")
print("=" * 60)
```

**Answer:**

The bivariate regression of Anxiety on StressSurvey produces the following estimated coefficients:

- **Intercept (Œ≤‚ÇÄ):** The intercept value from the regression output above
- **Coefficient on StressSurvey (Œ≤‚ÇÅ):** The slope coefficient from the regression output above
- **R-squared:** The R¬≤ value indicating model fit

**Comparison to True Relationship:**

The true relationship is: **Anxiety = Stress + 0.1 √ó Time**, where:
- The true coefficient on Stress (Œ≤‚ÇÅ) = **1.0**
- The true intercept (Œ≤‚ÇÄ) = **0**

However, when we regress Anxiety on StressSurvey (a proxy for Stress), we find:

1. **The estimated coefficient differs from 1.0:** The regression produces a coefficient that is not equal to 1.0, even though StressSurvey is intended to measure the same underlying construct as Stress.

2. **Why the discrepancy occurs:**
   - **Omitted variable bias:** This bivariate regression excludes Time, which is part of the true relationship. The omission of Time causes the coefficient on StressSurvey to be biased.
   - **Proxy measurement error:** StressSurvey is a non-linear proxy for the true Stress variable. While StressSurvey and Stress have a monotonic relationship, the non-linearity means that a one-unit increase in StressSurvey does not correspond to a one-unit increase in Stress. This measurement error propagates through the regression, causing the coefficient estimate to deviate from the true value.
   - **Non-linear relationship:** The relationship between StressSurvey and Anxiety is not perfectly linear, but linear regression forces a linear fit, leading to incorrect coefficient estimates.

3. **Key insight:** Even though StressSurvey appears to be a "good" proxy for Stress (monotonic relationship, high correlation), using it in a linear regression produces coefficient estimates that are systematically wrong. This demonstrates that **proxy variables with non-linear relationships to the true variables can lead to misleading regression results**, even when the proxy seems reasonable at first glance.

4. **The danger:** If a researcher were to interpret the estimated coefficient as the true effect of stress on anxiety, they would be making a fundamental error. The regression coefficient tells us about the relationship between StressSurvey and Anxiety, not the true relationship between Stress and Anxiety.

II. **Visualization of Bivariate Relationship:** Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.

```{python}
#| echo: false
#| label: bivariate-regression-stresssurvey-plot
#| fig-cap: "Scatter plot with regression line: Anxiety on StressSurvey"
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (8, 5)

# Create scatter plot with regression line
fig, ax = plt.subplots()
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1)

# Plot regression line
x_line = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_line = model_stresssurvey.predict(x_line.reshape(-1, 1))
ax.plot(x_line, y_line, color='red', linewidth=2, label=f'Regression line: y = {intercept:.3f} + {coefficient:.3f}x')

ax.set_xlabel('StressSurvey', fontsize=12)
ax.set_ylabel('Anxiety', fontsize=12)
ax.set_title('Bivariate Regression: Anxiety on StressSurvey', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nModel Fit Summary:")
print(f"  R¬≤ = {r_squared:.4f}")
print(f"  This means the model explains {r_squared*100:.2f}% of the variance in Anxiety")
```

**Analysis of Fit and Potential Issues:**

The scatter plot reveals several important insights about the bivariate regression of Anxiety on StressSurvey:

1. **Visual Fit Assessment:** The regression line appears to capture the general upward trend in the data, with most points clustering relatively close to the line. However, careful inspection shows that the relationship may not be perfectly linear.

2. **R-squared Interpretation:** The R¬≤ value from the regression output indicates how well the model fits the data. While a high R¬≤ suggests a reasonably good fit, it does not guarantee that the coefficient estimates are correct or that the relationship is truly linear.

3. **Key Issues to Consider:**
   - **Omitted Variable Bias:** This bivariate regression excludes Time, which is part of the true relationship (Anxiety = Stress + 0.1 √ó Time). This omission could bias the coefficient on StressSurvey.
   - **Proxy Variable Problem:** StressSurvey is a proxy for the true Stress variable. The non-linear relationship between StressSurvey and the actual Stress level means that even if Stress has a linear relationship with Anxiety, StressSurvey may not.
   - **Non-Linearity:** The relationship between StressSurvey and Anxiety may be non-linear, but linear regression forces a linear fit. This could lead to misleading coefficient estimates.
   - **Coefficient Interpretation:** The estimated coefficient differs from the true Stress coefficient of 1.0. This discrepancy suggests that using StressSurvey as a proxy introduces measurement error that affects the regression results.

4. **What the Visualization Tells Us:** The scatter plot allows us to visually inspect whether the linear assumption holds. If we see systematic patterns in the residuals (points consistently above or below the line in certain regions), this would indicate non-linearity that the regression model cannot capture. The visualization helps us understand that even when a regression appears to fit well (high R¬≤), the underlying relationship may not be linear, leading to incorrect coefficient estimates.

III. **Bivariate Regression Analysis with Time:** Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Bivariate regression: Anxiety ~ Time
import statsmodels.api as sm
X_time = observDF[['Time']]

y_anxiety = observDF['Anxiety'].values
# Using sklearn
model3_sklearn = LinearRegression()
model3_sklearn.fit(X_time, y_anxiety)

# Using statsmodels for detailed output
X_time_sm = sm.add_constant(X_time)
model3_sm = sm.OLS(y_anxiety, X_time_sm).fit()

# Print results
print("=" * 60)
print("BIVARIATE REGRESSION: Anxiety ~ Time")
print("=" * 60)
print(f"\nIntercept (beta_0): {model3_sklearn.intercept_:.4f}")
print(f"Coefficient on Time (beta_2): {model3_sm.params['Time']:.4f}")  
print(f"R-squared: {r2_score(y_anxiety, model3_sklearn.predict(X_time)):.4f}")  
print("\n" + "=" * 60)
print("STATSMODELS DETAILED OUTPUT:")
print("=" * 60)
print(model3_sm.summary())
```
**Answer:**
Bivariate regression of Anxiety ~ Time gives an intercept around -3.68 and a slope on Time about 5.34, with R¬≤ ‚âà 0.56. The true data-generating process only assigns 0.1 to Time, so this single-variable regression wildly overstates the time effect. The distortion comes from omitted-variable bias: stress drives both anxiety and time in the dataset, and once you leave stress out, the regression loads that shared variation onto Time, yielding the misleading 5.34 estimate.

**Comparison to True Relationship:**

The true relationship is: **Anxiety = Stress + 0.1 √ó Time**, where:
- The true coefficient on Time (beta_2) = **0.1**
- The true intercept (beta_0) = **0**

However, when we regress Anxiety on Time, we find:

1. **The estimated coefficient differs from 0.1:** The regression produces a coefficient that is not equal to 0.1, even though Time is part of the true relationship.

2. **Why the discrepancy occurs:**
   - **Omitted variable bias:** This bivariate regression excludes Stress, which is part of the true relationship. The omission of Stress causes the coefficient on Time to be biased.
   - **Non-linear relationship:** The relationship between Time and Anxiety is not perfectly linear, but linear regression forces a linear fit, leading to incorrect coefficient estimates.

3. **Key insight:** Even though Time appears to be a "good" predictor of Anxiety (linear relationship, high correlation), using it in a linear regression produces coefficient estimates that are systematically wrong. This demonstrates that **predictors with non-linear relationships to the true variable can lead to misleading regression results**, even when the predictor seems reasonable at first glance.

4. **The danger:** If a researcher were to interpret the estimated coefficient as the true effect of time on anxiety, they would be making a fundamental error. The regression coefficient tells us about the relationship between Time and Anxiety, not the true relationship between Stress and Anxiety.

IV. **Visualization of Bivariate Relationship:** Create a scatter plot with the regression line showing the relationship between Time and Anxiety.

```{python}
#| echo: false
#| label: bivariate-regression-time-plot
#| fig-cap: "Scatter plot with regression line: Anxiety on Time"
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (8, 5)

# Create scatter plot with regression line
fig, ax = plt.subplots()
ax.scatter(observDF['Time'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1)

# Plot regression line
x_line = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_line = model3_sklearn.predict(x_line.reshape(-1, 1))
ax.plot(x_line, y_line, color='red', linewidth=2, label=f'Regression line: y = {intercept:.3f} + {coefficient:.3f}x')

ax.set_xlabel('Time', fontsize=12)
ax.set_ylabel('Anxiety', fontsize=12)
ax.set_title('Bivariate Regression: Anxiety on Time', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nModel Fit Summary:")
print(f"  R¬≤ = {r_squared:.4f}")
print(f"  This means the model explains {r_squared*100:.2f}% of the variance in Anxiety")

```
Answer:
The scatter plot shows a positive relationship between Time and Anxiety, with most points clustering relatively close to the regression line. The R¬≤ value indicates that the model explains about 56% of the variance in Anxiety.

V. **Multiple Regression Analysis:** Run a multiple regression of Anxiety on both StressSurvey and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
#| label: multiple-regression-stresssurvey-time
#| fig-cap: "Multiple regression: Anxiety on StressSurvey and Time"
import statsmodels.api as sm

X_multiple = observDF[['StressSurvey', 'Time']]
y_anxiety = observDF['Anxiety'].values

# Using sklearn
model4_sklearn = LinearRegression()
model4_sklearn.fit(X_multiple, y_anxiety)

# Using statsmodels for detailed output
X_multiple_sm = sm.add_constant(X_multiple)
model4_sm = sm.OLS(y_anxiety, X_multiple_sm).fit()

# Print results
print("=" * 60)
print("MULTIPLE REGRESSION: Anxiety on StressSurvey and Time")
print("=" * 60)
print(f"\nIntercept (beta_0): {model4_sklearn.intercept_:.4f}")
print(f"Coefficient on StressSurvey (beta_1): {model4_sm.params['StressSurvey']:.4f}")
print(f"Coefficient on Time (beta_2): {model4_sm.params['Time']:.4f}")
print(f"R-squared: {r2_score(y_anxiety, model4_sklearn.predict(X_multiple)):.4f}")
print("\n" + "=" * 60)
print("STATSMODELS DETAILED OUTPUT:")
print("=" * 60)
print(model4_sm.summary())
```
**Answer:**
Those figures are the fitted statistics for the multiple regression Anxiety ~ StressSurvey + Time:
Intercept ‚âà 0.5888
StressSurvey coefficient ‚âà 1.4269
Time coefficient ‚âà -2.7799
R¬≤ ‚âà 0.9350
The true data-generating process was Anxiety = Stress + 0.1 √ó Time, so both slope estimates are off: the proxy StressSurvey overstates the stress effect (1.4269 vs 1.0), and the time coefficient flips sign and becomes large and negative instead of the true -2.7799. The high R¬≤ just means the model still fits the sample well, not that the coefficients are trustworthy; the non-linear proxy and the collinearity between stress and time push the interpretation in the wrong direction.

**Comparison to True Relationship:**
Multiple regression of Anxiety on StressSurvey and Time gives an intercept around 0.5888, a coefficient on StressSurvey around 1.4269, and a coefficient on Time around -2.7799, with R¬≤ ‚âà 0.9350. The true data-generating process assigns 1 to Stress, 0.1 to Time, and the intercept is 0, so this multiple regression captures both stress and time effects. The coefficients are close to the true values, and the model explains about 93.5% of the variance in Anxiety.  

**Key Questions:**

- Are your estimated coefficients close to these true values?
 No. Using StressSurvey and Time, we got roughly Œ≤‚ÇÄ ‚âà 0.59, Œ≤‚ÇÅ ‚âà 1.43, Œ≤‚ÇÇ ‚âà -2.78, while the truth is 0, 1, 0.1. The proxy is nonlinear, so the stress effect inflates and the time effect flips sign.

- If not, what does this tell you about the reliability of your regression model?
 When key inputs are measured with nonlinear proxies (or drivers are omitted), the regression picks up proxy quirks instead of the real relationship. Statistical significance doesn‚Äôt rescue those coefficients‚Äîthey‚Äôre biased.
- Even if your R-squared is high, are the coefficients telling the right story?
  No. R¬≤ measures fit, not truth. Even if a model fits well, its coefficients can be wrong if the relationship is non-linear or if key drivers are omitted.


### Questions to Answer for 85% Grade on Challenge

VI. **Multiple Regression Analysis:** Run a multiple regression of Anxiety on both **Stress** and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
#| label: multiple-regression-stress-time
#| fig-cap: "Multiple regression: Anxiety on Stress and Time"
import statsmodels.api as sm
X_stress_time = observDF[['Stress', 'Time']]
y_anxiety = observDF['Anxiety'].values

# Using sklearn
model5_sklearn = LinearRegression()
model5_sklearn.fit(X_stress_time, y_anxiety)

# Using statsmodels for detailed output
X_stress_time_sm = sm.add_constant(X_stress_time)
model5_sm = sm.OLS(y_anxiety, X_stress_time_sm).fit()

# Print results
print("=" * 60)
print("MULTIPLE REGRESSION: Anxiety ~ Stress + Time")
print("=" * 60)
print(f"\nIntercept (beta_0): {model5_sklearn.intercept_:.4f}")
print(f"Coefficient on Stress (beta_1): {model5_sm.params['Stress']:.4f}")
print(f"Coefficient on Time (beta_2): {model5_sm.params['Time']:.4f}")
print(f"R-squared: {r2_score(y_anxiety, model5_sklearn.predict(X_stress_time)):.4f}")
print("\n" + "=" * 60)
print("STATSMODELS DETAILED OUTPUT:")
print("=" * 60)
print(model5_sm.summary())
```
**Answer:**
Those figures are the fitted statistics for the multiple regression Anxiety ~ Stress + Time:
Intercept ‚âà -0.0000
Stress coefficient ‚âà 1.0000
Time coefficient ‚âà 0.1000
R¬≤ ‚âà 1.0000
The true data-generating process was Anxiety = Stress + 0.1 √ó Time, so both slope estimates are off: the stress coefficient overstates the stress effect (1.0000 vs 1.0), and the time coefficient is small and positive instead of the true 0.1000. The high R¬≤ just means the model still fits the sample well, not that the coefficients are trustworthy; the collinearity between stress and time pushes the interpretation in the wrong direction.

**Comparison to True Relationship:**

Multiple regression of Anxiety on Stress and Time gives an intercept around -0.0000, a coefficient on Stress around 1.0000, and a coefficient on Time around 0.1000, with R¬≤ ‚âà 1.0000. The true data-generating process assigns 1 to Stress, 0.1 to Time, and the intercept is 0, so this multiple regression captures both stress and time effects. The coefficients are close to the true values, and the model explains about 100% of the variance in Anxiety.
   

VII. **Model Comparison:** Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?

```{python}
#| echo: false
#| label: model-comparison
#| fig-cap: "Comparing StressSurvey vs Stress multiple regressions"

import pandas as pd

models = [
    {
        "name": "Anxiety ~ StressSurvey + Time",
        "X": observDF[['StressSurvey', 'Time']],
        "tags": ["proxy + time"]
    },
    {
        "name": "Anxiety ~ Stress + Time",
        "X": observDF[['Stress', 'Time']],
        "tags": ["true stress + time"]
    }
]

comparison_rows = []

for model_info in models:
    X = model_info["X"]
    X_sm = sm.add_constant(X)
    model_sm = sm.OLS(y_anxiety, X_sm).fit()
    row = {
        "Model": model_info["name"],
        "R_squared": model_sm.rsquared,
        "Adj_R_squared": model_sm.rsquared_adj,
        "Const_est": model_sm.params["const"],
        "Const_pvalue": model_sm.pvalues["const"]
    }
    for col in X.columns:
        row[f"{col}_est"] = model_sm.params[col]
        row[f"{col}_pvalue"] = model_sm.pvalues[col]
    row["Tags"] = ", ".join(model_info["tags"])
    comparison_rows.append(row)

comparison_df = pd.DataFrame(comparison_rows)
comparison_df
```
### Questions to Answer for 95% Grade on Challenge

VIII. **Reflect on Real-World Implications:** For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press.  What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model?  Assuming confirmation bias is real, which model is a typical parent going to believe?  Which model will Facebook, Instagram, and TikTok executives prefer?
Model 1 (using StressSurvey + Time) produces a big negative slope on time, so the popular press headline would read something like ‚ÄúMore Minutes on Social Media Slash Anxiety Levels.‚Äù
Model 2 (using true Stress + Time) recovers the tiny positive 0.1 slope, so you‚Äôd see ‚ÄúStress Drives Anxiety; Social Media Time Has Only a Minor Effect.‚Äù
A parent already wary of social media will latch onto the dramatic first headline because it locks in their confirmation bias (either ‚Äúsee, heavy use makes kids calmer‚Äù or ‚Äúthe study is obviously flawed‚Äù). The social platforms prefer the second model: it says their apps barely move the needle once stress is measured correctly.


### Questions to Answer for 100% Grade on Challenge

IX. **Avoiding Misleading Statistical Significance:** Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why?  Did you get results that are both statistically significant and close to the true relationship?
```{python}
#| echo: false
#| label: subset-regression
#| fig-cap: "Subset regression: StressSurvey ‚â§ 6"
subset = observDF[observDF["StressSurvey"] <= 6]

X_subset = subset[["StressSurvey", "Time"]]
y_subset = subset["Anxiety"].values

model_subset = sm.OLS(y_subset, sm.add_constant(X_subset)).fit()
print(model_subset.summary())
```
**Answer:**
I split the data at StressSurvey ‚â§ 6, keeping only the low-to-moderate regime where the survey proxy is almost linear, ran Anxiety ~ StressSurvey + Time, and got coefficients close to the truth with both slopes statistically significant‚Äîshowing that a smart subset (plus checking the proxy‚Äôs shape) prevents the misleading negative time effect we saw in the full samples


::: {.callout-tip}
## üéØ For 100% Grade: Focus on What's Most Interesting

**The key insight:** Linear regression can give you statistically significant results that are completely wrong. The challenge is understanding when and why this happens.

**What to investigate:**

- **Coefficient Interpretation:** What do the regression coefficients actually mean in this context?
- **The Problem of Non-Linearity:** Can adding variables to a regression equation flip the sign of a coefficient while still making it appear significant?

**Write like a data science consultant:** Your report should help someone understand not just what the numbers show, but why they're dangerous and what to do about it.
:::

## Essential Regression Concepts üéØ {#sec-regression-concepts}

Before diving into the challenge, let's review the key regression concepts you'll need. These examples will prepare you for the garbage can regression analysis.

### 1. Simple Linear Regression: The Basics

Let's start with a basic linear regression to understand the mechanics:

```{python}
#| label: simple-regression-python
#| fig-cap: Python simple linear regression example
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(123)

# Create simple example data
n = 50
x = np.random.normal(10, 3, n)
y = 2 * x + 3 + np.random.normal(0, 2, n)

# Fit linear regression
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)

# Display results
print(f"Coefficient: {model.coef_[0]:.3f}")
print(f"Intercept: {model.intercept_:.3f}")
print(f"R-squared: {r2_score(y, model.predict(x.reshape(-1, 1))):.3f}")

# Create scatter plot with regression line
fig, ax = plt.subplots(figsize=(7, 4))
ax.scatter(x, y, alpha=0.7)
ax.plot(x, model.predict(x.reshape(-1, 1)), color='red', linewidth=2)
ax.set_title('Simple Linear Regression')
ax.set_xlabel('X Variable')
ax.set_ylabel('Y Variable')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 2. Multiple Regression: Adding Complexity

Now let's see how multiple variables interact:

```{python}
#| label: multiple-regression-python
#| fig-cap: Python multiple regression example
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(456)

# Create multiple regression example
n = 50
x1 = np.random.normal(10, 3, n)
x2 = np.random.normal(5, 2, n)
y = 2 * x1 + 0.5 * x2 + 3 + np.random.normal(0, 2, n)

# Fit multiple regression
X = np.column_stack([x1, x2])
model_multi = LinearRegression()
model_multi.fit(X, y)

# Display results
print(f"Coefficients: {model_multi.coef_}")
print(f"Intercept: {model_multi.intercept_:.3f}")
print(f"R-squared: {r2_score(y, model_multi.predict(X)):.3f}")

# Create pairs plot
data_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
sns.pairplot(data_df)
plt.suptitle('Pairs Plot: Multiple Regression Variables', y=1.02)
plt.tight_layout()
plt.show()
```

::: {.callout-note}
## Statistical Significance at 5% Level

A coefficient is **statistically significant** when its p-value is less than 0.05.

- **p < 0.05**: Statistically significant
- **p ‚â• 0.05**: Not statistically significant

### Understanding Scientific Notation in P-values

Sometimes you'll see p-values written in scientific notation like `7.89e-4`. Don't panic! This is just a way to write very small numbers:

- **7.89e-4** means 7.89 √ó 10‚Åª‚Å¥ = 0.000789
- **2.34e-6** means 2.34 √ó 10‚Åª‚Å∂ = 0.00000234
- **1.23e-2** means 1.23 √ó 10‚Åª¬≤ = 0.0123

**The key rule:** If you see "e-" in a p-value, it's always a very small number (less than 1). The number after "e-" tells you how many zeros come before the first non-zero digit.

**Examples:**
- 7.89e-4 = 0.000789 (less than 0.05, so significant!)
- 2.34e-6 = 0.00000234 (way less than 0.05, so very significant!)
- 1.23e-2 = 0.0123 (less than 0.05, so significant!)

**Remember:** Statistical significance doesn't mean the effect is large or important - it just means we're confident the effect isn't zero.
:::

## The Problem of Non-Linearity: A Deeper Look

The "garbage can regression" problem occurs when we include variables in our regression models that create misleading results, even when they appear statistically significant. This happens in several ways:

1. **Random correlations:** Even random variables can appear correlated by chance
2. **Overfitting:** More variables can improve fit without improving understanding
3. **Multiple testing:** The more variables we test, the more likely we are to find spurious relationships
4. **Non-linear relationships:** Variables with U-shaped, exponential, or other non-linear relationships with the outcome are forced into a linear framework, creating misleading coefficients

### Why This Matters

In the real world, non-linear relationships can lead to:

- **False policy recommendations:** Basing decisions on spurious correlations or false causal relationships
- **Wasted resources:** Pursuing interventions that don't actually work
- **Loss of credibility:** When results can't be replicated or don't make sense
- **Ethical issues:** Making decisions that affect people's lives based on bad science

### The Solution

The key is to always ask:

1. **Does this make theoretical sense?** Is there a plausible mechanism?
2. **Is the relationship robust?** Does it hold across different samples and specifications?
3. **Are we overfitting?** Do we have enough data relative to the number of variables?
4. **Can we interpret the coefficients?** Do the results tell a coherent story?
5. **Is the relationship truly linear?** Check for non-linear patterns that linear regression can't capture
6. **Are we forcing the wrong functional form?** Consider if polynomial terms, interactions, or transformations are needed
7. **Split the sample into meaningful subsets:** Analyze different "statistical regimes" to see if relationships hold consistently across different parts of your data
8. **Use graphical diagnostics:** Don't rely blindly on "canned" regressions‚Äîvisualize the relationships to understand what's really happening

Remember: **Correlation is not causation, and regression coefficients can lie!** üìä
